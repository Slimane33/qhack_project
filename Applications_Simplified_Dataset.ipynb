{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wired-stadium",
   "metadata": {},
   "source": [
    "# Applications of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-structure",
   "metadata": {},
   "source": [
    "### Imports of utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "static-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from config import config\n",
    "from utils import circuit_final, encode_words\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "qml.enable_tape()\n",
    "num_words = config['NUM_WORDS']\n",
    "qbits_per_word = config['QUBITS_PER_WORDS']\n",
    "num_layers = config['NUM_LAYERS']\n",
    "\n",
    "\n",
    "my_bucket = f\"amazon-braket-edb2457fc968\" # the name of the bucket\n",
    "my_prefix = \"Variational-NLP\" # the name of the folder in the bucket\n",
    "s3_folder = (my_bucket, my_prefix)\n",
    "\n",
    "device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "julian-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 2**qbits_per_word\n",
    "max_length = num_words\n",
    "\n",
    "embeddings = np.load(\"dummy_dataset/embeddings.npy\")\n",
    "sentences = np.load(\"dummy_dataset/dummy_sentences_5.npy\").astype(int)\n",
    "#labels = np.load('newsgroup/labels.npy')\n",
    "\n",
    "np.random.seed(143)\n",
    "missing_word = np.random.randint(0, num_words, size=len(sentences)).astype(int)#.numpy()\n",
    "\n",
    "np.random.seed(32)\n",
    "np.random.shuffle(sentences)\n",
    "\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "pca = PCA(n_dim)\n",
    "embeddings_reduced = np.zeros((embeddings.shape[0], n_dim))\n",
    "embeddings_reduced[norms>0] = pca.fit_transform(embeddings[norms>0])\n",
    "\n",
    "norms_reduced = np.linalg.norm(embeddings_reduced, axis=1).reshape(-1,1)\n",
    "embeddings_reduced_norm = np.zeros_like(embeddings_reduced)#.numpy()\n",
    "embeddings_reduced_norm[norms>0] = embeddings_reduced[norms>0] / np.repeat(norms_reduced[norms>0], n_dim, axis=1)\n",
    "\n",
    "embeddings_reduced_norm.requires_grad = False\n",
    "sentences_truncated = sentences[:,0:max_length]\n",
    "sentences_truncated.requires_grad = False\n",
    "\n",
    "missing_word.requires_grad = False\n",
    "\n",
    "all_indices = np.repeat(np.arange(max_length).reshape((1,-1)), len(sentences), axis=0).astype(int)#.numpy()\n",
    "for i in range(len(sentences)):\n",
    "    all_indices[i, missing_word[i]] = max_length\n",
    "all_indices.requires_grad = False\n",
    "\n",
    "with open('dummy_dataset/vocab.p', 'rb') as readfile:\n",
    "    vocab = pickle.load(readfile)\n",
    "\n",
    "word_to_id = vocab\n",
    "id_to_word = {value:key for key,value in vocab.items() if np.linalg.norm(embeddings_reduced_norm[int(value)])>0}\n",
    "\n",
    "word_indices = np.array(list(id_to_word.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wires = qbits_per_word * (num_words+1) + 1\n",
    "\n",
    "dev_remote = qml.device(\n",
    "    \"braket.aws.qubit\",\n",
    "    device_arn=device_arn,\n",
    "    wires=n_wires,\n",
    "    s3_destination_folder=s3_folder,\n",
    "    parallel=True\n",
    ")\n",
    "\n",
    "dev_local = qml.device(\"braket.local.qubit\", wires=n_wires, shots=1000)\n",
    "\n",
    "dev = dev_local\n",
    "#dev = dev_remote\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def compute_overlap_words(parameters, embeddings, indices, target_word, wires=dev.wires):\n",
    "    encode_words(embeddings, indices)\n",
    "    params = [(parameters[:,0,i], parameters[:,1::,i]) for i in range(num_layers)]\n",
    "    circuit_final(params, wires, num_layers, target_word)\n",
    "    return qml.expval(qml.PauliZ(wires[-1]))\n",
    "\n",
    "\n",
    "def cost(parameters, sentences, missing_words):\n",
    "    cost = 0    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        embeddings = embeddings_reduced_norm[sentence]\n",
    "        indices = all_indices[i]\n",
    "        m_w = missing_words[i]\n",
    "        cost += compute_overlap_words(parameters, embeddings, indices, target_word = m_w)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-moldova",
   "metadata": {},
   "source": [
    "### Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "prospective-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = np.random.rand(qbits_per_word, int(np.ceil(num_words/2))+1, num_layers)\n",
    "parameters = np.load('saved_parameters/5_words/params_3_430.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "patent-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'man', 'chef', 'policeman', 'dog', 'cat', 'apple', 'fish', 'teacher', 'toy', 'kid', 'vegetable', 'doctor', 'car', 'boat', 'bird', 'meat', 'professor', 'president', 'student', 'chair', 'table', 'big', 'old', 'young', 'tiny', 'long', 'heavy', 'blue', 'strong', 'red', 'discret', 'tender', 'rotten', 'gentle', 'funny', 'sad', 'light', 'complex', 'green', 'cheap', 'expensive', 'eat', 'cut', 'cook', 'burn', 'fix', 'repair', 'build', 'hit', 'take', 'make', 'bake', 'paint', 'throw', 'push', 'create', 'look', 'pick', 'chop']\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-choir",
   "metadata": {},
   "source": [
    "## Predict the missing word\n",
    "\n",
    "The objective is to fill a blank in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "quiet-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most probable words are: \n",
      "woman cut man fix throw\n"
     ]
    }
   ],
   "source": [
    "def get_most_probable_word(sentence, position, look_in=None):\n",
    "    assert position<num_words\n",
    "    if look_in is None:\n",
    "        look_in = np.arange(len(word_indices)).astype(int)\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(num_words):\n",
    "        if i!=position:\n",
    "            indices.append(i)\n",
    "    indices.append(num_words)       \n",
    "    probas = []\n",
    "    embeddings_input = embeddings_reduced_norm[sentence]\n",
    "    for i,index in enumerate(word_indices[look_in]):\n",
    "        embeddings = np.concatenate([embeddings_input, embeddings_reduced_norm[index].reshape((1,-1))], axis=0)\n",
    "        probas.append(float(compute_overlap_words(parameters, embeddings, indices, target_word = position)))\n",
    "    return probas\n",
    "\n",
    "input_sentence = 'funny [mask] eat cheap vegetable'\n",
    "\n",
    "list_words = input_sentence.split(' ')\n",
    "list_index = []\n",
    "missing_index = 0\n",
    "for i,word in enumerate(list_words):\n",
    "    if word=='[mask]':\n",
    "        missing_index = i\n",
    "    else:\n",
    "        list_index.append(vocab[word])\n",
    "\n",
    "np.random.seed(23)\n",
    "#look_in = np.random.randint(len(word_indices), size=10).astype(int)\n",
    "p = get_most_probable_word(list_index, missing_index, look_in=None)\n",
    "\n",
    "#print(np.argsort(p)[::-1])\n",
    "\n",
    "print(\"The 5 most probable words are: \")\n",
    "print(' '.join(id_to_word[int(i)] for i in np.argsort(p)[::-1][0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-adoption",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sitting-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strong kid tiny light boat\n"
     ]
    }
   ],
   "source": [
    "sentence = 'strong kid'\n",
    "\n",
    "for i in range(len(sentence.split(' ')), num_words):\n",
    "    list_words = sentence.split(' ')\n",
    "    list_index = []\n",
    "    missing_index = 0\n",
    "    for i,word in enumerate(list_words):\n",
    "        list_index.append(vocab[word])\n",
    "\n",
    "    look_in = None#np.random.randint(len(word_indices), size=3).astype(int)\n",
    "    p = get_most_probable_word(list_index, len(list_words), look_in=look_in)\n",
    "    \n",
    "    new_word = id_to_word[int(np.argmax(p))]\n",
    "    \n",
    "    sentence = sentence + ' ' + new_word\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-panel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
