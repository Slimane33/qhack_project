{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Language Model\n",
    "**QHack hackathon | 22-26 february 2021**\\\n",
    "*By `TeamX` Slimane Thabet & Jonas Landman* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "In this project, we developed a variational quantum algorithm for **Natural Language Processing** (NLP). Our goal is to **train a quantum circuit such that it can process and recognize words**. Applications varies from **word matching**, **sentence completion**, **sentence generation** and more.\n",
    "\n",
    "---\n",
    "#### Word encoding\n",
    "Words are preprocessed using state-of-the art deep learning **word embedding** methods such as FastText. Then these embeddings arer cast down to few features using dimensionality reduction. For instance each word will be described as a vector of 8 dimensions. Using **Quantum Amplitude Encoding**, we can encode each word into a 3-qubits register. If a **sentence** is composed of $N$ words, and to represent it we propose to stack $N$ 3-qubits register sequentially.\n",
    "\n",
    "#### Variational Circuit\n",
    "We propose a new ansatz and training methodology to perform this NLP quantum learning:  \n",
    "- The ansatz is composed of several layers of controlled rotations that mix the words between each other, and between themselves. \n",
    "- During the training, we will **mask one word randomly in each sentence**, by imposing its quantum register to  $|0\\rangle$\n",
    "- Using a **SWAP Test**, a supplementary word is then compared to the output register of the missing word (after the output of the ansatz). Therefore the cost function is the probability of output '0' on the swap test's ancillary qubit. We chose the supplementary word to be the missing word itself in order to drive the learning. \n",
    "- The goal of the training is to adjust the ansatz's parameters such that **the missing word is guessed**. \n",
    "\n",
    "<img src=\"circuit.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "#### Applications\n",
    "With such a circuit trained, we can provide a new sentence with a missing word and compare it with all possible words in the \"dictionary\". We can generate artifical sentence by starting with only one word, or completing a sentence after its last words. \n",
    "\n",
    "<img src=\"sentence_generation.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "#### Performances\n",
    "We consider $M$ sentences of $N$ words, each one encoded as $Q$ qubits. \n",
    "- **Number of qubits required**: One quantum circuit corresponds to one sentence plus an extra word and an ancillary qubit, therefore $Q*(N+1)+1$ qubits. E.g for a 4 words sentence with 3 qubits per words, we require 16 qubits. For a 5 words sentence with 4 qubits per words, we require 25 qubits. \n",
    "- **Number of trainable parameters**: The number of trainable parameters in the ansatz is around $Q*(1+N/2)*L$, where $L$ is the number of layers, on average (it depends of the parity of the number of words, and number of qubits). E.g for a 4 words sentence with 3 qubits per words and 3 layers, we require 27 parameters.\n",
    "\n",
    "We can use AWS SV1 for parallelizing the gradient during the training. But the computational cost remains high due to the number of sentences and the total number of words in the dictionary. \n",
    "\n",
    "#### Datasets\n",
    "We propose 3 differents datasets to train and test our algorithm\n",
    "- **IMDB Dataset** composed of (?) sentences and (?) words in total\n",
    "- **Newsgroup Dataset** composed of (?) sentences and (?) words in total\n",
    "- An **synthetic dataset** of 'dummy' sentences with small number of sentences and words, for performance limitation and grammatical simplicity\n",
    "\n",
    "\n",
    "#### Code architecture\n",
    "- The **Pennylane** variational ansatz are defined in `utils.py`\n",
    "- The NLP preprocessing using FastText is made in `embeddings.ipynb` and generate readable file as `embeddings.npy`, `sentences.npy` etc.\n",
    "- In `config.py` are defined the global configurations such as the number of words, of qubit per words, and the number of layers per ansatz.\n",
    "- In this notebook, we train the quantum variational circuit and test applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from config import config\n",
    "from utils import circuit_final, encode_words\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "qml.enable_tape()\n",
    "num_words = config['NUM_WORDS']\n",
    "qbits_per_word = config['QUBITS_PER_WORDS']\n",
    "num_layers = config['NUM_LAYERS']\n",
    "\n",
    "\n",
    "my_bucket = f\"amazon-braket-edb2457fc968\" # the name of the bucket\n",
    "my_prefix = \"Variational-NLP\" # the name of the folder in the bucket\n",
    "s3_folder = (my_bucket, my_prefix)\n",
    "\n",
    "device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence preprocessing\n",
    "Load embeddings and sentence, apply dimensionality reduction, randomly defined missing words in each sentence, generate the input structure for our variational quantum circuit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 2**qbits_per_word\n",
    "max_length = num_words\n",
    "\n",
    "embeddings = np.load(\"newsgroup/embeddings.npy\")\n",
    "sentences = np.load(\"newsgroup/sentences.npy\").astype(int)\n",
    "labels = np.load('newsgroup/labels.npy')\n",
    "\n",
    "np.random.seed(143)\n",
    "missing_word = np.random.randint(0, num_words, size=len(sentences)).astype(int)#.numpy()\n",
    "\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "pca = PCA(n_dim)\n",
    "embeddings_reduced = np.zeros((embeddings.shape[0], n_dim))\n",
    "embeddings_reduced[norms>0] = pca.fit_transform(embeddings[norms>0])\n",
    "\n",
    "norms_reduced = np.linalg.norm(embeddings_reduced, axis=1).reshape(-1,1)\n",
    "embeddings_reduced_norm = np.zeros_like(embeddings_reduced)#.numpy()\n",
    "embeddings_reduced_norm[norms>0] = embeddings_reduced[norms>0] / np.repeat(norms_reduced[norms>0], n_dim, axis=1)\n",
    "\n",
    "embeddings_reduced_norm.requires_grad = False\n",
    "sentences_truncated = sentences[:,0:max_length]\n",
    "sentences_truncated.requires_grad = False\n",
    "\n",
    "missing_word.requires_grad = False\n",
    "\n",
    "all_indices = np.repeat(np.arange(max_length).reshape((1,-1)), len(sentences), axis=0).astype(int)#.numpy()\n",
    "for i in range(len(sentences)):\n",
    "    all_indices[i, missing_word[i]] = max_length\n",
    "all_indices.requires_grad = False\n",
    "\n",
    "with open('newsgroup/vocab.p', 'rb') as readfile:\n",
    "    vocab = pickle.load(readfile)\n",
    "\n",
    "word_to_id = vocab\n",
    "id_to_word = {value:key for key,value in vocab.items() if np.linalg.norm(embeddings_reduced_norm[int(value)])>0}\n",
    "\n",
    "word_indices = list(id_to_word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  january comet within million march 1986 spacecraft carrying three measure interplanetary magnetic\n",
      "Truncated sentence:  january comet within million march 1986 spacecraft\n"
     ]
    }
   ],
   "source": [
    "print(\"Original sentence: \", ' '.join(id_to_word[int(id)] for id in sentences[10]))\n",
    "print(\"Truncated sentence: \", ' '.join(id_to_word[int(id)] for id in sentences_truncated[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between ' medicine ' and ' disease ' in original fasttext embedding:  0.5201646458004967\n",
      "Similarity between ' medicine ' and ' january ' in original fasttext embedding:  0.12138553809780182\n",
      "\n",
      "Similarity between ' medicine ' and ' disease ' in reduced normalized embedding:  0.7680937936597123\n",
      "Similarity between ' medicine ' and ' january ' in reduced normalized embedding:  0.281465338256116\n"
     ]
    }
   ],
   "source": [
    "word1 = 'medicine'\n",
    "word2 = 'disease'\n",
    "word3 = 'january'\n",
    "\n",
    "id1, id2, id3 = vocab[word1], vocab[word2], vocab[word3]\n",
    "e1, e2, e3 = embeddings[[id1, id2, id3]]\n",
    "er1, er2, er3 = embeddings_reduced_norm[[id1, id2, id3]]\n",
    "\n",
    "print(\"Similarity between '\", word1, \"' and '\", word2, \"' in original fasttext embedding: \", np.abs(np.dot(e1, e2))/(np.linalg.norm(e1)*np.linalg.norm(e2)))\n",
    "print(\"Similarity between '\", word1, \"' and '\", word3, \"' in original fasttext embedding: \", np.abs(np.dot(e1, e3))/(np.linalg.norm(e1)*np.linalg.norm(e3)))\n",
    "print(\"\")\n",
    "print(\"Similarity between '\", word1, \"' and '\", word2, \"' in reduced normalized embedding: \", np.abs(np.dot(er1, er2)))\n",
    "print(\"Similarity between '\", word1, \"' and '\", word3, \"' in reduced normalized embedding: \", np.abs(np.dot(er1, er3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "The circuit is defined as `circuit_final(params, wires, num_layers, target_word)` in `utils.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wires = qbits_per_word * (max_length+1) + 1\n",
    "\n",
    "dev_remote = qml.device(\n",
    "    \"braket.aws.qubit\",\n",
    "    device_arn=device_arn,\n",
    "    wires=n_wires,\n",
    "    s3_destination_folder=s3_folder,\n",
    "    parallel=True\n",
    ")\n",
    "\n",
    "dev_local = qml.device(\"default.qubit\", wires=n_wires)\n",
    "\n",
    "#dev = dev_local\n",
    "dev = dev_remote\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def compute_overlap_words(parameters, embeddings, indices, target_word, wires=dev.wires):\n",
    "    encode_words(embeddings, indices)\n",
    "    params = [(parameters[:,0,i], parameters[:,1::,i]) for i in range(num_layers)]\n",
    "    circuit_final(params, wires, num_layers, target_word)\n",
    "    return qml.expval(qml.PauliZ(wires[-1]))\n",
    "\n",
    "\n",
    "def cost(parameters, sentences, missing_words):\n",
    "    cost = 0    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        embeddings = embeddings_reduced_norm[sentence]\n",
    "        indices = all_indices[i]\n",
    "        m_w = missing_words[i]\n",
    "        cost += compute_overlap_words(parameters, embeddings, indices, target_word = m_w)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 2\n",
    "N_batches = len(sentences_truncated)//batch_size\n",
    "\n",
    "parameters = np.random.rand(qbits_per_word, int(np.ceil(num_words/2))+1, num_layers)\n",
    "\n",
    "opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(N_batches):\n",
    "        t0 = time()\n",
    "        batch = np.arange(i*batch_size, (i+1)*batch_size).astype(int)\n",
    "        \n",
    "        def cost_batch(parameters):\n",
    "            return cost(parameters, sentences_truncated[batch], missing_word[batch])\n",
    "        \n",
    "        parameters = opt.step(cost_batch, parameters)\n",
    "        t1 = time()\n",
    "        print('Time: ', t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_most_probable_word(sentence, position, look_in=None):\n",
    "    assert position<num_words\n",
    "    if look_in is None:\n",
    "        look_in = np.arange(len(word_indices)).astype(int)\n",
    "    indices = []\n",
    "    for i in range(num_words):\n",
    "        if i!=position:\n",
    "            indices.append(i)\n",
    "    indices.append(num_words)       \n",
    "    probas = []\n",
    "    embeddings_input = embeddings_reduced_norm[sentence]\n",
    "    for i,index in enumerate(word_indices[look_in]):\n",
    "        embeddings = np.concatenate([embeddings_input, embeddings_reduced_norm[index].reshape((1,-1))], axis=0)\n",
    "        probas.append(float(compute_overlap_words(parameters, embeddings, indices, target_word = position)))\n",
    "    return probas\n",
    "\n",
    "input_sentence = 'january [mask] within million march 1986 spacecraft'\n",
    "\n",
    "list_words = input_sentence.split(' ')\n",
    "list_index = []\n",
    "missing_index = 0\n",
    "for i,word in enumerate(list_words):\n",
    "    if word=='[mask]':\n",
    "        missing_index = i\n",
    "    else:\n",
    "        list_index.append(vocab[word])\n",
    "\n",
    "np.random.seed(23)\n",
    "look_in = np.random.randint(len(word_indices), size=10).astype(int)\n",
    "p = get_most_probable_word(list_index, 4, look_in=look_in)\n",
    "\n",
    "print(\"The 5 most probable words are: \")\n",
    "print(' '.join(id_to_word[int(look_in[i])] for i in np.argsort(p)[::-1][0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
