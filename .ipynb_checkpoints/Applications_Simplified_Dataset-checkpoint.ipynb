{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports of utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from config import config\n",
    "from utils import circuit_final, encode_words, entity_recognition_decoder, Ansatz_circuit\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "qml.enable_tape()\n",
    "num_words = config['NUM_WORDS']\n",
    "qbits_per_word = config['QUBITS_PER_WORDS']\n",
    "num_layers = config['NUM_LAYERS']\n",
    "\n",
    "\n",
    "my_bucket = f\"amazon-braket-edb2457fc968\" # the name of the bucket\n",
    "my_prefix = \"Variational-NLP\" # the name of the folder in the bucket\n",
    "s3_folder = (my_bucket, my_prefix)\n",
    "\n",
    "device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 2**qbits_per_word\n",
    "max_length = num_words\n",
    "\n",
    "embeddings = np.load(\"dummy_dataset/embeddings.npy\")\n",
    "sentences = np.load(\"dummy_dataset/dummy_sentences_5.npy\").astype(int)\n",
    "#labels = np.load('newsgroup/labels.npy')\n",
    "\n",
    "np.random.seed(143)\n",
    "missing_word = np.random.randint(0, num_words, size=len(sentences)).astype(int)#.numpy()\n",
    "\n",
    "np.random.seed(32)\n",
    "np.random.shuffle(sentences)\n",
    "\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "pca = PCA(n_dim)\n",
    "embeddings_reduced = np.zeros((embeddings.shape[0], n_dim))\n",
    "embeddings_reduced[norms>0] = pca.fit_transform(embeddings[norms>0])\n",
    "\n",
    "norms_reduced = np.linalg.norm(embeddings_reduced, axis=1).reshape(-1,1)\n",
    "embeddings_reduced_norm = np.zeros_like(embeddings_reduced)#.numpy()\n",
    "embeddings_reduced_norm[norms>0] = embeddings_reduced[norms>0] / np.repeat(norms_reduced[norms>0], n_dim, axis=1)\n",
    "\n",
    "embeddings_reduced_norm.requires_grad = False\n",
    "sentences_truncated = sentences[:,0:max_length]\n",
    "sentences_truncated.requires_grad = False\n",
    "\n",
    "missing_word.requires_grad = False\n",
    "\n",
    "all_indices = np.repeat(np.arange(max_length).reshape((1,-1)), len(sentences), axis=0).astype(int)#.numpy()\n",
    "for i in range(len(sentences)):\n",
    "    all_indices[i, missing_word[i]] = max_length\n",
    "all_indices.requires_grad = False\n",
    "\n",
    "with open('dummy_dataset/vocab.p', 'rb') as readfile:\n",
    "    vocab = pickle.load(readfile)\n",
    "\n",
    "word_to_id = vocab\n",
    "id_to_word = {value:key for key,value in vocab.items() if np.linalg.norm(embeddings_reduced_norm[int(value)])>0}\n",
    "\n",
    "word_indices = np.array(list(id_to_word.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wires = qbits_per_word * (num_words+1) + 1\n",
    "\n",
    "dev_remote = qml.device(\n",
    "    \"braket.aws.qubit\",\n",
    "    device_arn=device_arn,\n",
    "    wires=n_wires,\n",
    "    s3_destination_folder=s3_folder,\n",
    "    parallel=True\n",
    ")\n",
    "\n",
    "dev_local = qml.device(\"braket.local.qubit\", wires=n_wires, shots=1000)\n",
    "\n",
    "dev = dev_local\n",
    "#dev = dev_remote\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def compute_overlap_words(parameters, embeddings, indices, target_word, wires=dev.wires):\n",
    "    encode_words(embeddings, indices)\n",
    "    params = [(parameters[:,0,i], parameters[:,1::,i]) for i in range(num_layers)]\n",
    "    circuit_final(params, wires, num_layers, target_word)\n",
    "    return qml.expval(qml.PauliZ(wires[-1]))\n",
    "\n",
    "\n",
    "def cost(parameters, sentences, missing_words):\n",
    "    cost = 0    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        embeddings = embeddings_reduced_norm[sentence]\n",
    "        indices = all_indices[i]\n",
    "        m_w = missing_words[i]\n",
    "        cost += compute_overlap_words(parameters, embeddings, indices, target_word = m_w)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = np.random.rand(qbits_per_word, int(np.ceil(num_words/2))+1, num_layers)\n",
    "parameters = np.load('saved_parameters/5_words/params_4_0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'man', 'chef', 'policeman', 'dog', 'cat', 'apple', 'fish', 'teacher', 'toy', 'kid', 'vegetable', 'doctor', 'car', 'boat', 'bird', 'meat', 'professor', 'president', 'student', 'chair', 'table', 'big', 'old', 'young', 'tiny', 'long', 'heavy', 'blue', 'strong', 'red', 'discret', 'tender', 'rotten', 'gentle', 'funny', 'sad', 'light', 'complex', 'green', 'cheap', 'expensive', 'eat', 'cut', 'cook', 'burn', 'fix', 'repair', 'build', 'hit', 'take', 'make', 'bake', 'paint', 'throw', 'push', 'create', 'look', 'pick', 'chop']\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the missing word\n",
    "\n",
    "The objective is to fill a blank in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most probable words are: \n",
      "bird cat create repair dog\n"
     ]
    }
   ],
   "source": [
    "def get_most_probable_word(sentence, position, look_in=None):\n",
    "    assert position<num_words\n",
    "    if look_in is None:\n",
    "        look_in = np.arange(len(word_indices)).astype(int)\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(num_words):\n",
    "        if i!=position:\n",
    "            indices.append(i)\n",
    "    indices.append(num_words)       \n",
    "    probas = []\n",
    "    embeddings_input = embeddings_reduced_norm[sentence]\n",
    "    for i,index in enumerate(word_indices[look_in]):\n",
    "        embeddings = np.concatenate([embeddings_input, embeddings_reduced_norm[index].reshape((1,-1))], axis=0)\n",
    "        probas.append(float(compute_overlap_words(parameters, embeddings, indices, target_word = position)))\n",
    "    return probas\n",
    "\n",
    "input_sentence = 'funny [mask] eat cheap vegetable'\n",
    "\n",
    "list_words = input_sentence.split(' ')\n",
    "list_index = []\n",
    "missing_index = 0\n",
    "for i,word in enumerate(list_words):\n",
    "    if word=='[mask]':\n",
    "        missing_index = i\n",
    "    else:\n",
    "        list_index.append(vocab[word])\n",
    "\n",
    "np.random.seed(23)\n",
    "#look_in = np.random.randint(len(word_indices), size=10).astype(int)\n",
    "p = get_most_probable_word(list_index, missing_index, look_in=None)\n",
    "\n",
    "#print(np.argsort(p)[::-1])\n",
    "\n",
    "print(\"The 5 most probable words are: \")\n",
    "print(' '.join(id_to_word[int(i)] for i in np.argsort(p)[::-1][0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to generate a full sentence given the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence generated: \n",
      "funny professor green cat man\n"
     ]
    }
   ],
   "source": [
    "sentence = 'funny professor'\n",
    "\n",
    "for i in range(len(sentence.split(' ')), num_words):\n",
    "    list_words = sentence.split(' ')\n",
    "    list_index = []\n",
    "    missing_index = 0\n",
    "    for i,word in enumerate(list_words):\n",
    "        list_index.append(vocab[word])\n",
    "\n",
    "    look_in = None#np.random.randint(len(word_indices), size=3).astype(int)\n",
    "    p = get_most_probable_word(list_index, len(list_words), look_in=look_in)\n",
    "    \n",
    "    new_word = id_to_word[int(np.argmax(p))]\n",
    "    \n",
    "    sentence = sentence + ' ' + new_word\n",
    "    \n",
    "print('Sentence generated: ')   \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of Named Entity Recognition is to distiguish named entitities from other categories of words. Here we don't have entities in our dataset, but we can train our model to recognize verbs. It requires trianing a second ansatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wires = qbits_per_word * num_words\n",
    "n_layers_decoder = 3\n",
    "parameters_decoder = np.random.rand(n_layers_decoder, qbits_per_word)\n",
    "parameters_encoder = np.load('saved_parameters/dummy_dataset/5_words/params_4_40.npy')\n",
    "parameters_encoder.requires_grad = False\n",
    "\n",
    "dev_local = qml.device(\"braket.local.qubit\", wires=n_wires, shots=1000)\n",
    "\n",
    "dev = dev_local\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def compute_proba_entity(parameters_decoder, parameters_encoder, embeddings, indices, wires=dev.wires):\n",
    "    encode_words(embeddings, indices)\n",
    "    params_encoder = [(parameters_encoder[:,0,i], parameters_encoder[:,1::,i]) for i in range(num_layers)]\n",
    "    Ansatz_circuit(params_encoder, wires, num_layers)\n",
    "    for i in range(num_words):\n",
    "        entity_recognition_decoder(parameters_decoder, wires[i*qbits_per_word:(i+1)*qbits_per_word])\n",
    "    list_wires = [wires[qbits_per_word] * i for i in range(num_words)]\n",
    "    return [qml.probs(wire) for wire in list_wires]\n",
    "\n",
    "def cost_entity(parameters_decoder, parameters_encoder, sentences, labels):\n",
    "    cost = 0    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        embeddings = embeddings_reduced_norm[sentence]\n",
    "        indices = np.arange(num_words).astype(int)\n",
    "        label = labels[i]\n",
    "        probas = compute_proba_entity(parameters_decoder, parameters_encoder, embeddings, indices)[:,1]\n",
    "        cost += np.sum((probas-labels)**2)\n",
    "    return cost\n",
    "\n",
    "labels = np.zeros_like(sentences_truncated)\n",
    "labels[:,2] = 1\n",
    "labels.requires_grad = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  big student create green boat\n",
      "Labels:  [0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence: ', ' '.join(id_to_word[int(i)] for i in sentences_truncated[50]))\n",
    "print('Labels: ', labels[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.514, 0.151, 0.35 , 0.473, 0.703], requires_grad=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentences_truncated[59]\n",
    "indices = np.arange(num_words).astype(int)\n",
    "\n",
    "embeddings = embeddings_reduced_norm[sentence]\n",
    "\n",
    "compute_proba_entity(parameters_decoder, parameters_encoder, embeddings, indices, wires=dev.wires)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 8\n",
    "N_train = 5000\n",
    "N_batches = N_train//batch_size\n",
    "\n",
    "parameters_decoder = np.random.rand(n_layers_decoder, qbits_per_word)\n",
    "parameters_encoder = np.load('saved_parameters/dummy_dataset/5_words/params_4_40.npy')\n",
    "parameters_encoder.requires_grad = False\n",
    "\n",
    "opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(N_batches):\n",
    "        t0 = time()\n",
    "        batch = np.arange(i*batch_size, (i+1)*batch_size).astype(int)\n",
    "        \n",
    "        def cost_entity_batch(parameters_decoder):\n",
    "            return cost_entity(parameters_decoder, parameters_encoder, sentences_truncated[batch], labels[batch])\n",
    "        \n",
    "        parameters_decoder, loss = opt.step_and_cost(cost_entity_batch, parameters_decoder)\n",
    "        losses.append(loss)\n",
    "        t1 = time()\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for a new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_decoder = np.load('saved_parameters/dummy_dataset/decoder/params_0_90.npy')\n",
    "parameters_encoder = np.load('saved_parameters/dummy_dataset/5_words/params_4_40.npy')\n",
    "\n",
    "def get_entities(sentence):\n",
    "    embeddings = embeddings_reduced_norm[sentence]\n",
    "    indices = np.arange(num_words).astype(int)\n",
    "    p = compute_proba_entity(parameters_decoder, parameters_encoder, embeddings, indices)[:,1]\n",
    "    return 1*(p>=0.5)\n",
    "\n",
    "def compute_accuracy(sentences, labels):\n",
    "    acc = 0\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        acc += np.sum(np.abs(get_entities(sentence)-labels[i]))\n",
    "    acc = 1 - acc / (sentences.shape[0] * sentences.shape[1])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  old president repair heavy table\n",
      "Predictions:  [0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence: ', ' '.join(id_to_word[int(i)] for i in sentences_truncated[143]))\n",
    "print('Predictions: ', get_entities(sentences_truncated[143]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.42052, requires_grad=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(sentences[0:5000], labels[0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
